<!DOCTYPE html>
<html><head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Can Rationalization Improve Robustness? - Karthik Narasimhan&#39;s Lab</title><link rel="icon" type="image/png" href="/images/favicon.ico">

	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="A growing line of work has investigated the development of neural NLP models that can produce rationalesâ€“subsets of input that can explain their model predictions. In this paper, we ask whether such rationale models can also provide robustness to adversarial attacks in addition to their interpretable nature. Since these models need to first generate rationales (&ldquo;rationalizer&rdquo;) before making predictions (&ldquo;predictor&rdquo;), they have the potential to ignore noise or adversarially added text by simply masking it out of the generated rationale." />
	<meta property="og:image" content=""/>
	<meta property="og:title" content="Can Rationalization Improve Robustness?" />
<meta property="og:description" content="A growing line of work has investigated the development of neural NLP models that can produce rationalesâ€“subsets of input that can explain their model predictions. In this paper, we ask whether such rationale models can also provide robustness to adversarial attacks in addition to their interpretable nature. Since these models need to first generate rationales (&ldquo;rationalizer&rdquo;) before making predictions (&ldquo;predictor&rdquo;), they have the potential to ignore noise or adversarially added text by simply masking it out of the generated rationale." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/publications/chen-22-rational/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-04-25T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Can Rationalization Improve Robustness?"/>
<meta name="twitter:description" content="A growing line of work has investigated the development of neural NLP models that can produce rationalesâ€“subsets of input that can explain their model predictions. In this paper, we ask whether such rationale models can also provide robustness to adversarial attacks in addition to their interpretable nature. Since these models need to first generate rationales (&ldquo;rationalizer&rdquo;) before making predictions (&ldquo;predictor&rdquo;), they have the potential to ignore noise or adversarially added text by simply masking it out of the generated rationale."/>
<script src="http://example.org/js/feather.min.js"></script>
	
	
        <link href="http://example.org/css/fonts.1f07d2398f08441d78602c672fb8cb2a0e9a2395c03b8d547ffbf1c8787e4dc6.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="http://example.org/css/main.a7c0f144c33cc899db2a6bf2ed45af562fb88f5e218ca285f11e73d18f555b64.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="http://example.org/css/dark.2e992fe869b44f61fe1cc8a1ffb970084d2fc4a2bfb9d0f30cece368875edd65.css"  disabled />
	
	
	
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="http://example.org/">Karthik Narasimhan&#39;s Lab</a>
	</div>
	<nav>
		
		<a href="/posts">All Posts</a>&nbsp;&nbsp;&nbsp;
		
		<a href="/publications">Publications</a>&nbsp;&nbsp;&nbsp;
		
		<a href="/people">People</a>&nbsp;&nbsp;&nbsp;
		
		<a href="/tags">Tags</a>&nbsp;&nbsp;&nbsp;
		
		| <a id="dark-mode-toggle" onclick="toggleTheme()" href="">&nbsp;&nbsp;&nbsp;</a>
		<script src="http://example.org/js/themetoggle.js"></script>
		
	</nav>
</header>


<main>
	<article class="content-box">
		<div>
			<h1 style="margin-top:0.5em" class="title" >Can Rationalization Improve Robustness?</h1>
			<div class="meta">Posted on Apr 25, 2022</div>
			<div style="padding-top:10px"><span>
						<a href="https://howard50b.github.io/"><img class="author-list-photo"
								src="/images/people/hc22.png"
							/>
						<span class="author-list-name" margin="0" padding="0">Howard Chen</span></a>
					</span>
					&nbsp;<span>
						<a href="https://jacqueline-he.github.io/"><defaultphoto style="font-size: 0.94em">ðŸ˜Š</defaultphoto> 
						
						<span class="author-list-name" margin="0" padding="0">Jacqueline He</span></a>
					</span>
					&nbsp;<span>
						<a href="https://www.cs.princeton.edu/~karthikn/"><img class="author-list-photo"
								src="/images/people/karthikn.jpeg"
							/>
						<span class="author-list-name" margin="0" padding="0">Karthik Narasimhan</span></a>
					</span>
					&nbsp;<span>
						<a href="https://www.cs.princeton.edu/~danqic/"><img class="author-list-photo"
								src="/images/people/danqi.jpeg"
							/>
						<span class="author-list-name" margin="0" padding="0">Danqi Chen</span></a>
					</span>
					&nbsp;</div><div style="padding-top:10px;"><span class="supp-tag">
					<a href="https://github.com/princeton-nlp/rationale-robustness">GitHub</a>
				</span>
				<span class="supp-tag">
					<a href="https://arxiv.org/abs/2204.11790">Paper</a>
				</span>
				</div>
		</div>
		<hr class="post-separator"/>
		<section class="body post-content">
			<p>A growing line of work has investigated the development of neural NLP models that can produce rationalesâ€“subsets of input that can explain their model predictions. In this paper, we ask whether such rationale models can also provide robustness to adversarial attacks in addition to their interpretable nature. Since these models need to first generate rationales (&ldquo;rationalizer&rdquo;) before making predictions (&ldquo;predictor&rdquo;), they have the potential to ignore noise or adversarially added text by simply masking it out of the generated rationale. To this end, we systematically generate various types of â€™AddTextâ€™ attacks for both token and sentence-level rationalization tasks, and perform an extensive empirical evaluation of state-of-the-art rationale models across five different tasks. Our experiments reveal that the rationale models show the promise to improve robustness, while they struggle in certain scenariosâ€“when the rationalizer is sensitive to positional bias or lexical choices of attack text. Further, leveraging human rationale as supervision does not always translate to better performance. Our study is a first step towards exploring the interplay between interpretability and robustness in the rationalize-then-predict framework.</p>

		</section>
	</article>
	
	<script
		type="text/javascript"
		async
		src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"
	>
	</script>
</main>



<script>
  feather.replace()
</script></div>
    </body>
</html>
